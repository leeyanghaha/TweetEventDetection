# echo 1 > /proc/sys/vm/drop_caches

pip3 install nltk
pip3 install wordsegment
pip3 install python-Levenshtein
# nltk_data	E:\DOWNLOADS\BaiduYun\nltk_data.tar.gz
# http://python.jobbole.com/81675/

pip3 install spacy
pip3 install ..../en_core_web_lg-2.0.0.tar.gz
(python -m spacy download en_core_web_lg) OR
(https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.0.0/en_core_web_lg-2.0.0.tar.gz)

# bzip2 -z FileName
# for file in *.gz; do gunzip -xf $file; done
# for file in *.json; do bzip2 -z $file; done


* 数据预处理：
    * 网络用语和不规范用语的规范化处理
        * Lexical Normalisation for English Tweets—评测任务和相关文章汇总：
            * https://noisy-text.github.io/norm-shared-task.html
        * Twitter Text Normalization:
            * https://github.com/gouwsmeister/TextCleanser
    * 分词、词性标注、命名实体识别等：
        * CMU的TweetNLP:
            * http://www.cs.cmu.edu/~ark/TweetNLP/
            * 提供了分词、词性标注、语法分析
        * 华盛顿大学的Twitter_NLP：
            * https://github.com/aritter/twitter_nlp
            * 提供命名实体识别